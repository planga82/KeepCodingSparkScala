{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo Spark & Scala\n",
    "\n",
    "## Spark Core\n",
    "#### @FJPiqueras - KeepCoding\n",
    "\n",
    "#### A tener en cuenta que los notebooks de jupyter generan por defecto:\n",
    "\n",
    "##### SparkContext en la variable sc\n",
    "##### SparkSession en la variable spark\n",
    "\n",
    "##### Notebook powered by Spark 2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de primer RDD con un dataset numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)\n",
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[188] at parallelize at <console>:29\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Generación de dataset\n",
    "val data = (1 to 100).toList\n",
    "\n",
    "//Creación de RDD\n",
    "val rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getNumPartitions - Obtener número particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: Int = 2\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count - Cuenta el tamaño del rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Long = 100\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cache - Cachea el rdd en memoria para que al reutilizarse los datos queden cargados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: rdd.type = ParallelCollectionRDD[0] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take - Acción que devuelve el número de registros que se le pase por parámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Obtiene el número de líneas que se le pasan por parámetro\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first - Obtiene el primer elemento de un rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res54: Int = 1\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top y takeOrdered - Obtienen muestras de rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res62: Array[Int] = Array(100, 99, 98, 97, 96)\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Los últimos 5 guardados\n",
    "rdd.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res61: Array[Int] = Array(1, 2, 3, 4, 5)\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Cinco elementos ordenados\n",
    "rdd.takeOrdered(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach - Recorre el rdd para realizar posteriores operaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "//Recorrer un RDD\n",
    "rdd.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "-----\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "rdd.take(10).foreach(println(_))\n",
    "\n",
    "println(\"-----\")\n",
    "\n",
    "//Lo mismo con una lambda\n",
    "rdd.take(10).foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map - Transformación de mapeo de rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n",
      "102\n",
      "104\n",
      "106\n",
      "108\n",
      "110\n",
      "112\n",
      "114\n",
      "116\n",
      "118\n",
      "120\n",
      "122\n",
      "124\n",
      "126\n",
      "128\n",
      "130\n",
      "132\n",
      "134\n",
      "136\n",
      "138\n",
      "140\n",
      "142\n",
      "144\n",
      "146\n",
      "148\n",
      "150\n",
      "152\n",
      "154\n",
      "156\n",
      "158\n",
      "160\n",
      "162\n",
      "164\n",
      "166\n",
      "168\n",
      "170\n",
      "172\n",
      "174\n",
      "176\n",
      "178\n",
      "180\n",
      "182\n",
      "184\n",
      "186\n",
      "188\n",
      "190\n",
      "192\n",
      "194\n",
      "196\n",
      "198\n",
      "200\n",
      "-------\n",
      "102\n",
      "104\n",
      "106\n",
      "108\n",
      "110\n",
      "112\n",
      "114\n",
      "116\n",
      "2\n",
      "4\n",
      "118\n",
      "120\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "122\n",
      "124\n",
      "126\n",
      "128\n",
      "130\n",
      "132\n",
      "14\n",
      "16\n",
      "18\n",
      "134\n",
      "136\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "138\n",
      "140\n",
      "142\n",
      "144\n",
      "146\n",
      "148\n",
      "150\n",
      "152\n",
      "154\n",
      "156\n",
      "158\n",
      "28\n",
      "30\n",
      "32\n",
      "160\n",
      "162\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "164\n",
      "166\n",
      "168\n",
      "170\n",
      "42\n",
      "44\n",
      "46\n",
      "172\n",
      "174\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "176\n",
      "178\n",
      "180\n",
      "182\n",
      "184\n",
      "186\n",
      "188\n",
      "190\n",
      "192\n",
      "194\n",
      "196\n",
      "198\n",
      "200\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "rdd.map(_ * 2).foreach(println(_))\n",
    "\n",
    "println(\"-------\")\n",
    "\n",
    "rdd.map(x => x*2).foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter - Realiza un filtrado sobre el rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "//Filtrado de los números impares calculando el módulo\n",
    "rdd.filter(x => {x%2 ==0}).foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct - Filtra los elementos distintos de un rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "listaDistintos: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[13] at parallelize at <console>:27\n",
       "res5: Array[String] = Array(a, a, b)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val listaDistintos = sc.parallelize(List(\"a\",  \"a\", \"b\"))\n",
    "\n",
    "listaDistintos.distinct.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect - transforma un RDD en un objeto Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "listaDistintos.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap - Aplana el rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val x = sc.parallelize(List(\"spark_rdd_example\",  \"KeepCoding_mola\"), 2)\n",
    "\n",
    "val yMap = x.map(x => x.split(\"_\"))\n",
    "yMap.foreach(println(_))\n",
    "\n",
    "println(\"------\")\n",
    "//El map genera dos arrays con el resultado de cada uno de los elementos de la lista\n",
    "yMap.foreach(x => x.foreach(println(_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Sin embargo el map aplana el resultado dejándolo en la misma dimensión\n",
    "val yFlatMap = x.flatMap(x => x.split(\"_\"))\n",
    "yFlatMap.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textFile - Carga de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[String] = Datasets/FBI_CrimeDatasetNYC.csv MapPartitionsRDD[85] at textFile at <console>:28\n",
       "output: Array[String] = Array(\"DR Number,Date Reported,Date Occurred,Time Occurred,Area ID,Area Name,Reporting District,Crime Code,Crime Code Description,MO Codes,Victim Age,Victim Sex,Victim Descent,Premise Code,Premise Description,Weapon Used Code,Weapon Description,Status Code,Status Description,Crime Code 1,Crime Code 2,Crime Code 3,Crime Code 4,Address,Cross Street,Location \", 001208575,03/14/2013,03/11/2013,1800,12,77th Street,1241,626,INTIMATE PARTNER - SIMPLE ASSAULT,0416 0446 1243 2000,30,F,W,502,\"MULTI-UNIT DWELLING (APARTMENT, DUPLEX, ETC)\",400,\"STRONG-ARM (HANDS, FIST, FEET OR BODILY FORCE)\",AO,Adult Other,626,,,,6300    BRYNHURST                    AV,,..."
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Carga de dataset en RDD\n",
    "val data = sc.textFile(\"Datasets/FBI_CrimeDatasetNYC.csv\")\n",
    "val output = data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey - Reduce el rdd agrupándolo por cada clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b,30)\n",
      "(a,3)\n",
      "(c,4)\n",
      "-------\n",
      "(b,30)\n",
      "(a,3)\n",
      "(c,4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rddActions: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[8] at parallelize at <console>:29\n",
       "y: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[9] at reduceByKey at <console>:32\n",
       "y2: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[10] at reduceByKey at <console>:33\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddActions = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"c\", 4), (\"b\", 10), (\"b\", 18), (\"a\", 2)))\n",
    "\n",
    "val y = rddActions.reduceByKey((accum, n) => (accum + n))\n",
    "val y2 = rddActions.reduceByKey(_ + _)\n",
    "\n",
    "y.collect().foreach(println(_))\n",
    "println(\"-------\")\n",
    "y2.collect().foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduceY: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[198] at parallelize at <console>:27\n",
       "y3: Int = 55\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reduceY = sc.parallelize(1 to 10, 2)\n",
    "val y3 = reduceY.reduce((accum,n) => (accum + n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortBy - Ordenación de rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(c,4)\n",
      "(b,2)\n",
      "(b,10)\n",
      "(b,18)\n",
      "(a,1)\n",
      "(a,2)\n",
      "-----\n",
      "(a,1)\n",
      "(a,2)\n",
      "(b,2)\n",
      "(b,10)\n",
      "(b,18)\n",
      "(c,4)\n",
      "-----\n",
      "(a,1)\n",
      "(b,2)\n",
      "(a,2)\n",
      "(c,4)\n",
      "(b,10)\n",
      "(b,18)\n"
     ]
    }
   ],
   "source": [
    "//sortBy\n",
    "rddActions.sortBy(_._1, ascending = false).collect.foreach(println(_))\n",
    "println(\"-----\")\n",
    "rddActions.sortBy(_._1, ascending = true).collect.foreach(println(_))\n",
    "println(\"-----\")\n",
    "rddActions.sortBy(_._2, ascending = true).collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey - Ordenación de rdd por la clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(key2,2)\n",
      "(key1,1)\n",
      "(key1,3)\n",
      "------\n",
      "(key2,2)\n",
      "(key1,1)\n",
      "(key1,3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inputrdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[93] at parallelize at <console>:28\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//sortByKey\n",
    "val  inputrdd= sc.parallelize(Seq( (\"key1\", 1), (\"key2\", 2), (\"key1\", 3)))\n",
    "\n",
    "inputrdd.sortByKey(false).foreach(println(_))\n",
    "println(\"------\")\n",
    "inputrdd.sortByKey(true).foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey - Agrupación por la clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(key1,CompactBuffer(1, 3))\n",
      "(key2,CompactBuffer(2))\n"
     ]
    }
   ],
   "source": [
    "inputrdd.groupByKey.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey - Cuenta los elementos agrupado por la clave del rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res63: scala.collection.Map[String,Long] = Map(key1 -> 2, key2 -> 1)\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputrdd.countByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join - Realiza el conjunto de aquellos elementos que aparezcan en dos rdds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b,(2,10))\n",
      "(b,(10,10))\n",
      "(b,(18,10))\n",
      "(a,(1,1))\n",
      "(a,(2,1))\n",
      "-------\n",
      "(z,(4,None))\n",
      "(x,(18,None))\n",
      "(b,(10,Some(2)))\n",
      "(b,(10,Some(10)))\n",
      "(b,(10,Some(18)))\n",
      "(w,(2,None))\n",
      "(a,(1,Some(1)))\n",
      "(a,(1,Some(2)))\n",
      "(y,(2,None))\n",
      "-------\n",
      "(z,(None,4))\n",
      "(x,(None,18))\n",
      "(b,(Some(2),10))\n",
      "(b,(Some(10),10))\n",
      "(b,(Some(18),10))\n",
      "(w,(None,2))\n",
      "(a,(Some(1),1))\n",
      "(a,(Some(2),1))\n",
      "(y,(None,2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[177] at parallelize at <console>:31\n",
       "rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[178] at parallelize at <console>:32\n",
       "rddJoin: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[181] at join at <console>:34\n",
       "rddLeftJoin: org.apache.spark.rdd.RDD[(String, (Int, Option[Int]))] = MapPartitionsRDD[184] at leftOuterJoin at <console>:39\n",
       "rddRightJoin: org.apache.spark.rdd.RDD[(String, (Option[Int], Int))] = MapPartitionsRDD[187] at rightOuterJoin at <console>:44\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"c\", 4), (\"b\", 10), (\"b\", 18), (\"a\", 2)))\n",
    "val rdd2 = sc.parallelize(List((\"a\", 1), (\"y\", 2), (\"z\", 4), (\"b\", 10), (\"x\", 18), (\"w\", 2)))\n",
    "\n",
    "val rddJoin = rdd1.join(rdd2)\n",
    "rddJoin.collect.foreach(println(_))\n",
    "\n",
    "println(\"-------\")\n",
    "\n",
    "val rddLeftJoin = rdd2.leftOuterJoin(rdd1)\n",
    "rddLeftJoin.collect.foreach(println(_))\n",
    "\n",
    "println(\"-------\")\n",
    "\n",
    "val rddRightJoin = rdd1.rightOuterJoin(rdd2)\n",
    "rddRightJoin.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsTextFile - Guardado de RDD en fichero de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "30: error: not found: value data",
     "output_type": "error",
     "traceback": [
      "<console>:30: error: not found: value data",
      "       data.saveAsSequenceFile(\"Datasets/output/FBISeqFile\")",
      "       ^",
      "<console>:25: error: not found: value data",
      "       data.saveAsTextFile(\"Datasets/output/FBI.txt\")",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "data.saveAsTextFile(\"Datasets/output/FBI.txt\")\n",
    "\n",
    "//Guardado de RDD como fichero secuencial\n",
    "data.saveAsSequenceFile(\"Datasets/output/FBISeqFile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1  - Realiza WordCount del libro del quijote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[18] at reduceByKey at <console>:27\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Solucion\n",
    "val data = sc.textFile(\"Datasets/quijote.txt\")\n",
    "val counts = data.flatMap(line => line.split(\" \")).map(word => (word,1)).reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 - Obtiene las 10 palabras más usadas en el libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quixote: org.apache.spark.rdd.RDD[String] = Datasets/quijote.txt MapPartitionsRDD[200] at textFile at <console>:25\n",
       "result: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[208] at sortByKey at <console>:30\n",
       "res66: Array[(Int, String)] = Array((2226,como), (1419,para), (1189,porque), (1069,que,), (1006,había), (963,todo), (873,-dijo), (862,bien), (813,-respondió), (792,vuestra))\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val quixote = sc.textFile(\"Datasets/quijote.txt\") \n",
    "\n",
    "val result = quixote.flatMap(line => line.split(\" \")).\n",
    "filter(word => word.length>3 && !List(\"Quijote\",\"Sancho\").contains(word)).map(word => (word,1)).\n",
    "reduceByKey(_ + _).\n",
    "map(tuple => tuple.swap).sortByKey(false)\n",
    "\n",
    "result.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b,7)\n",
      "(a,9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pairs: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[52] at parallelize at <console>:28\n",
       "resReduce: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[53] at reduceByKey at <console>:29\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairs = sc.parallelize(Array((\"a\", 3), (\"a\", 1), (\"b\", 7), (\"a\", 5)))\n",
    "val resReduce = pairs.reduceByKey(_+_)\n",
    "\n",
    "resReduce.foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toDebugString - Muestra el plan de ejecución del RDD (útil para el tunning, lo veremos más adelante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res69: String =\n",
       "(2) ShuffledRDD[208] at sortByKey at <console>:30 []\n",
       " +-(2) MapPartitionsRDD[205] at map at <console>:30 []\n",
       "    |  ShuffledRDD[204] at reduceByKey at <console>:29 []\n",
       "    +-(2) MapPartitionsRDD[203] at map at <console>:28 []\n",
       "       |  MapPartitionsRDD[202] at filter at <console>:28 []\n",
       "       |  MapPartitionsRDD[201] at flatMap at <console>:27 []\n",
       "       |  Datasets/quijote.txt MapPartitionsRDD[200] at textFile at <console>:25 []\n",
       "       |  Datasets/quijote.txt HadoopRDD[199] at textFile at <console>:25 []\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones combinadadas de RDDs, Ordenación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res49: Array[Int] = Array(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710, 720, 730, 740, 750, 760, 770, 780, 790, 800, 810, 820, 830, 840, 850, 860, 870, 880, 890, 900, 910, 920, 930, 940, 950, 960, 970, 980, 990, 1000)\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddA = sc.parallelize((0 to 100).toList)\n",
    "val rddB = sc.parallelize((0 to 1000 by 10).toList)\n",
    "\n",
    "//Operaciones de combinado de RDDs\n",
    "//Ordenado del rdd < a >\n",
    "rddA.collect.sortWith(_<_)\n",
    "\n",
    "//Ordenado del rdd de > a <\n",
    "rddB.collect.sortWith(_<_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res50: Array[Int] = Array(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Interesección entre ambos rdds (Aquellos números que aparecen en ambos RDDs)\n",
    "rddA.intersection(rddB).collect.sortWith(_<_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: Array[Int] = Array(0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710, 720, 730, 740, 750, 760, 77..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Union de ambos rdds (Aparecen todos los números de ambos rdds)\n",
    "rddA.union(rddB).collect.sortWith(_<_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[109] at parallelize at <console>:25\n",
       "accounts: org.apache.spark.rdd.RDD[(Int, (String, Int))] = ParallelCollectionRDD[110] at parallelize at <console>:27\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users = sc.parallelize(List((1,\"John\"),(2,\"Mary\"),(3,\"Mark\"))) // (UserId,Name) pairs\n",
    "users.collect()\n",
    "val accounts = sc.parallelize(List((1,(\"#1\",1000)),(1,(\"#2\",500)),(2,(\"#3\",750)))) // (UserId, (AccountId,Salary)) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res54: Array[(Int, (String, Int))] = Array((1,(#1,1000)), (1,(#2,500)), (2,(#3,750)))\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,(Mary,(#3,750)))\n",
      "(1,(John,(#1,1000)))\n",
      "(1,(John,(#2,500)))\n"
     ]
    }
   ],
   "source": [
    "users.join(accounts).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quijote: org.apache.spark.rdd.RDD[String] = Dataset/quijote.txt MapPartitionsRDD[126] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Solución\n",
    "val quijote = sc.textFile(\"Dataset/quijote.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res58: Double = 6.522577710536595\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordLengths.foreach(length => accum += length)\n",
    "accum.value.toDouble / wordLengths.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)\n",
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at <console>:29\n",
       "res8: rdd.type = ParallelCollectionRDD[17] at parallelize at <console>:29\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = (1 to 10).toList\n",
    "\n",
    "//Creación de RDD\n",
    "val rdd = sc.parallelize(data)\n",
    "\n",
    "rdd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "targetfile: String = Dataset/targetList.txt\n",
       "res57: org.apache.spark.rdd.RDD[String] = Dataset/targetList.txt MapPartitionsRDD[124] at textFile at <console>:30\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val targetfile = \"Dataset/targetList.txt\"\n",
    "\n",
    "import scala.io.Source\n",
    "val targetlist = Source.fromFile(targetfile).getLines.toList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acumuladores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jpgcount: org.apache.spark.Accumulator[Int] = 0\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jpgcount = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3 - Calcula la longitud media de palabras del quijote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordLengths: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[214] at map at <console>:28\n",
       "accum: org.apache.spark.Accumulator[Int] = 0\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordLengths = quixote.flatMap(line => line.split(\" \")).\n",
    "                          filter(word => word.length > 3 && !List(\"Quixote\", \"Sancho\").contains(word)).map(_.length).cache\n",
    "val accum = sc.accumulator(0, \"AccumLength\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4 - Guarda los resultados del ejercicio anterior en un nuevo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordLengths.saveAsTextFile(\"Datasets/output/Ejercicio4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 (SPylon)",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
