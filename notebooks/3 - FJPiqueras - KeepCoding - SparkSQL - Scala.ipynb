{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo Spark & Scala\n",
    "\n",
    "## Spark SQL\n",
    "#### @FJPiqueras - KeepCoding\n",
    "\n",
    "#### A tener en cuenta que los notebooks de jupyter generan por defecto:\n",
    "\n",
    "##### SparkContext en la variable sc\n",
    "##### SparkSession en la variable spark\n",
    "\n",
    "##### Notebook powered by Spark 2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En primer lugar definimos las case classes con las que vamos a trabajar en todo el módulo. Se trata de un dataset de películas que contiene tres ficheros distintos. users.dat, movies.dat y ratings.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de dataset en formato json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import spark.implicits._\n",
    "\n",
    " val df = spark.read.json(\"Datasets/SparkSQL/personas.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printSchema - Muestra el schema del dataframe, al cargar un fichero de tipo json que trae un esquema se estructura automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primeras queries con la API SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  //show muestra por pantalla la información del dataframe en formato tabla\n",
    "  df.select(\"name\").show\n",
    "  df.select(\"name\", \"age\").show\n",
    "  df.select(\"*\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones simples con la api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.select($\"name\", $\"age\" + 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter - Filtrados, equivalente en sql a \"select * from personas p where age>21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.filter($\"age\" > 21).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where - Condiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(\"age>21\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limit - obtiene las filas indicadas por parámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(1).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumnRenamed - Renombrar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newDf=df.withColumnRenamed(\"age\", \"edad\")\n",
    "newDf.limit(1).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### order by - Ordenación por uno de los campos del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf.orderBy(\"edad\", \"name\").show\n",
    "newDf.sort(\"name\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### createOrReplaceTempView - Registra el dataframe en una tabla con el nombre pasado por parámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.createOrReplaceTempView(\"personas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De la API de Spark al uso de programación declarativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM personas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando queries sobre una nueva sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.newSession().sql(\"SELECT * FROM personas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries \"avanzadas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT name, age FROM personas WHERE age >= 13 AND age <= 19 and name like 'J%'\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries al vuelo directamente sobre fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM json.`Datasets/SparkSQL/personas.json`\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de ficheros estructurados dentro de una clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|idDpto|   name|\n",
      "+----+------+-------+\n",
      "|null|     1|Michael|\n",
      "|  30|     3|   Andy|\n",
      "|  19|     4| Justin|\n",
      "|null|     1| Javier|\n",
      "|  38|     2|  Laura|\n",
      "|  54|     5|Nicolas|\n",
      "|  34|  null| Raquel|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class Person\n",
       "import spark.implicits._\n",
       "peopleDS: org.apache.spark.sql.Dataset[Person] = [age: bigint, idDpto: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(name: String, age: Long)\n",
    "\n",
    "import spark.implicits._\n",
    "val peopleDS = spark.read.json(\"Datasets/SparkSQL/personas.json\").as[Person]\n",
    "\n",
    "peopleDS.select(\"*\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasando a datasets no estructurados a información estructurada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|idDpto|   name|\n",
      "+----+------+-------+\n",
      "|null|     1|Michael|\n",
      "|  30|     3|   Andy|\n",
      "|  19|     4| Justin|\n",
      "|null|     1| Javier|\n",
      "|  38|     2|  Laura|\n",
      "|  54|     5|Nicolas|\n",
      "|  34|  null| Raquel|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): java.lang.ClassCastException: Person cannot be cast to Person",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): java.lang.ClassCastException: Person cannot be cast to Person",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)",
      "  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:644)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:603)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:612)",
      "  ... 39 elided",
      "Caused by: java.lang.ClassCastException: Person cannot be cast to Person",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:108)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    " val peopleDF = spark.sparkContext\n",
    "      .textFile(\"Datasets/SparkSQL/personas.txt\")\n",
    "      .map(_.split(\",\")).map(row => Person(row(0), row(1).trim.toLong))\n",
    "      .toDF()\n",
    "\n",
    "peopleDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.select(\"*\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from people\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando maps sobre SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Obtén la edad de los teenagers de la tabla personas\n",
    "val dfTeenagers = spark.sql(\"select * from personas where age <20\").map(row => \"Name: \" + row(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones agregadas: avg, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf.groupBy().avg(\"edad\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf.groupBy(\"name\", \"edad\").count().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Funciones agregadas - max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf.agg(min(\"edad\")).show\n",
    "newDf.agg(max(\"edad\")).show\n",
    "\n",
    "//Equivalente con lenguaje declarativo\n",
    "newDf.createOrReplaceTempView(\"personasNew\")\n",
    "spark.sql(\"select max(edad) from personasNew\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión Dataframe en RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfToRDD: org.apache.spark.sql.DataFrame = [age: bigint, idDpto: bigint ... 1 more field]\n",
       "res12: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[63] at rdd at <console>:31\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfToRDD = spark.read.json(\"Datasets/SparkSQL/personas.json\")\n",
    "\n",
    "dfToRDD.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones con varios dataframes\n",
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|idDpto|   name|\n",
      "+----+------+-------+\n",
      "|null|     1|Michael|\n",
      "|  30|     3|   Andy|\n",
      "|  19|     4| Justin|\n",
      "|null|     1| Javier|\n",
      "|  38|     2|  Laura|\n",
      "|  54|     5|Nicolas|\n",
      "|  34|  null| Raquel|\n",
      "+----+------+-------+\n",
      "\n",
      "+---+--------------+\n",
      "| id|          name|\n",
      "+---+--------------+\n",
      "|  1|       BigData|\n",
      "|  2|     Analítica|\n",
      "|  3|      Business|\n",
      "|  4|          RRHH|\n",
      "|  5|OficinaTécnica|\n",
      "|  6|         Staff|\n",
      "+---+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfPersonas: org.apache.spark.sql.DataFrame = [age: bigint, idDpto: bigint ... 1 more field]\n",
       "dfDptos: org.apache.spark.sql.DataFrame = [id: bigint, name: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val dfPersonas = spark.read.json(\"Datasets/SparkSQL/personas.json\")\n",
    " val dfDptos = spark.read.json(\"Datasets/SparkSQL/departamentos.json\")\n",
    "\n",
    "dfPersonas.show\n",
    "dfDptos.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|   name|          name|\n",
      "+-------+--------------+\n",
      "|Michael|       BigData|\n",
      "|   Andy|      Business|\n",
      "| Justin|          RRHH|\n",
      "| Javier|       BigData|\n",
      "|  Laura|     Analítica|\n",
      "|Nicolas|OficinaTécnica|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfJoin: org.apache.spark.sql.DataFrame = [name: string, name: string]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfJoin = spark.sql(\"select p.name, d.name from personasJoin p, departamentosJoin d where p.idDpto=d.id\")\n",
    "dfJoin.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPersonas.createOrReplaceTempView(\"personasJoin\")\n",
    "dfDptos.createOrReplaceTempView(\"departamentosJoin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,IntegerType,Some(List(StringType)))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"strLen\", (s: String) =>  s.length())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|   name|UDF:strLen(name)|\n",
      "+-------+----------------+\n",
      "|Michael|               7|\n",
      "|   Andy|               4|\n",
      "| Justin|               6|\n",
      "| Javier|               6|\n",
      "|  Laura|               5|\n",
      "|Nicolas|               7|\n",
      "| Raquel|               6|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT name, strLen(name) from personasJoin\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save - Guardado de los resultados de un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTeenagers.drop(df.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactuando con Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//No funciona en jupyter por un problema con el hive metastore\n",
    "//val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"CREATE TABLE IF NOT EXISTS people (age INT, name STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH 'Datasets/Dataset2/ratings.data' INTO TABLE ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"FROM src SELECT key, value\").collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuación de ejercicios de SparkSQL en VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 (SPylon)",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
